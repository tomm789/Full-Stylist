

===== netlify.toml =====
[build]
  command = "npx expo export --platform web --output-dir web-build"
  publish = "web-build"
  functions = "netlify/functions"

[dev]
  # Use static framework - we only need functions, not a dev server
  framework = "#static"
  # Netlify dev server port
  port = 8888
  # Functions directory
  functions = "netlify/functions"
  # Disable auto-launch of browser
  autoLaunch = false
  # Serve public directory for static files
  publish = "public"

[functions]
  # Ensure TypeScript functions are included and compiled
  node_bundler = "esbuild"
  included_files = ["netlify/functions/**/*.ts"]
  # Use external dependencies (Supabase, etc.) - don't bundle them
  external_node_modules = ["@supabase/supabase-js"]

# AI job runner needs extended timeout for image generation (60-90 seconds)
[functions."ai-job-runner"]
  timeout = 120

[[redirects]]
  from = "/*"
  to = "/index.html"
  status = 200

# Headers for PWA and caching
[[headers]]
  # HTML files - no cache to ensure updates work
  for = "/*.html"
  [headers.values]
    Cache-Control = "no-cache, no-store, must-revalidate"
    Pragma = "no-cache"
    Expires = "0"

[[headers]]
  # Service worker - no cache to ensure updates
  for = "/service-worker.js"
  [headers.values]
    Cache-Control = "no-cache, no-store, must-revalidate"
    Pragma = "no-cache"
    Expires = "0"

[[headers]]
  # Manifest - short cache
  for = "/manifest.webmanifest"
  [headers.values]
    Cache-Control = "public, max-age=3600"

[[headers]]
  # Static assets (hashed) - long cache
  for = "/_expo/*"
  [headers.values]
    Cache-Control = "public, max-age=31536000, immutable"

[[headers]]
  # Icons and images - medium cache
  for = "/icons/*"
  [headers.values]
    Cache-Control = "public, max-age=86400"

[[headers]]
  # Other static assets - long cache
  for = "/assets/*"
  [headers.values]
    Cache-Control = "public, max-age=31536000, immutable"



===== package.json =====
{
  "name": "full-stylist",
  "version": "3.1.0",
  "description": "AI styling app for creating virtual headshots and outfits",
  "scripts": {
    "start": "expo start",
    "android": "expo start --android",
    "ios": "expo start --ios",
    "web": "expo start --web",
    "dev": "netlify dev"
  },
  "keywords": [
    "ai",
    "styling",
    "fashion",
    "virtual-stylist"
  ],
  "author": "",
  "license": "MIT",
  "dependencies": {
    "@expo/metro-runtime": "^6.1.2",
    "@react-native-async-storage/async-storage": "^2.2.0",
    "@supabase/ssr": "^0.8.0",
    "@supabase/supabase-js": "^2.90.1",
    "@types/react": "~19.1.10",
    "browser-image-compression": "^2.0.2",
    "date-fns": "^4.1.0",
    "dotenv": "^17.2.3",
    "expo": "^54.0.31",
    "expo-constants": "~18.0.13",
    "expo-file-system": "^19.0.21",
    "expo-image": "~3.0.11",
    "expo-image-picker": "^17.0.10",
    "expo-linking": "~8.0.11",
    "expo-router": "^6.0.21",
    "react": "19.1.0",
    "react-dom": "19.1.0",
    "react-easy-crop": "^5.5.6",
    "react-native": "0.81.5",
    "react-native-gesture-handler": "~2.28.0",
    "react-native-reanimated": "~4.1.1",
    "react-native-safe-area-context": "~5.6.0",
    "react-native-screens": "~4.16.0",
    "react-native-url-polyfill": "^3.0.0",
    "react-native-web": "^0.21.2",
    "react-native-worklets": "0.5.1",
    "sharp": "^0.34.5",
    "typescript": "^5.9.3"
  },
  "devDependencies": {
    "@netlify/functions": "^5.1.2",
    "babel-plugin-module-resolver": "^5.0.2"
  }
}



===== netlify/functions/ai-job-runner.js =====
"use strict";

// Main Netlify function for processing AI jobs. This handler
// authenticates the request using the Supabase JWT, retrieves the
// referenced job from the database and dispatches processing to the
// appropriate module based on the job type. Each job type is defined
// in a separate module under ./processes to improve modularity.

const { supabaseAdmin } = require("./supabaseClient");
const { createPerformanceTracker, createTimingTracker, downloadImageFromStorage } = require("./utils");
const { processAutoTag } = require("./processes/auto_tag");
const { processProductShot } = require("./processes/product_shot");
const { processHeadshotGenerate } = require("./processes/headshot_generate");
const { processBodyShotGenerate } = require("./processes/body_shot_generate");
const { processOutfitRender } = require("./processes/outfit_render");
const { processOutfitMannequin } = require("./processes/outfit_mannequin");
const { processOutfitSuggest } = require("./processes/outfit_suggest");
const { processReferenceMatch } = require("./processes/reference_match");

/**
 * Netlify function handler. Validates the HTTP method and the
 * Authorization header, retrieves the job from Supabase, and queues it
 * for asynchronous processing. Returns immediately with a 202 status.
 *
 * @param {object} event - The incoming request context
 * @param {object} context - The Lambda context provided by Netlify
 * @returns {Promise<object>} HTTP response object
 */
exports.handler = async (event, context) => {
  // Netlify runs all functions in a single Lambda; prevent waiting on
  // asynchronous tasks to finish before returning
  context.callbackWaitsForEmptyEventLoop = false;
  const headers = {
    "Access-Control-Allow-Origin": "*",
    "Access-Control-Allow-Headers": "Content-Type, Authorization",
    "Access-Control-Allow-Methods": "POST, OPTIONS",
    "Content-Type": "application/json"
  };
  // Handle CORS preflight
  if (event.httpMethod === "OPTIONS") {
    return { statusCode: 200, headers, body: "" };
  }
  if (event.httpMethod !== "POST") {
    return {
      statusCode: 405,
      headers,
      body: JSON.stringify({ error: "Method not allowed" })
    };
  }
  try {
    // Extract and verify JWT from Authorization header
    const authHeader = event.headers.authorization || event.headers.Authorization;
    if (!authHeader || !authHeader.startsWith("Bearer ")) {
      return {
        statusCode: 401,
        headers,
        body: JSON.stringify({ error: "Missing authorization header" })
      };
    }
    const token = authHeader.replace("Bearer ", "");
    const {
      data: { user },
      error: authError
    } = await supabaseAdmin.auth.getUser(token);
    if (authError || !user) {
      return {
        statusCode: 401,
        headers,
        body: JSON.stringify({ error: "Invalid token" })
      };
    }
    // Parse the job ID from the request body
    const body = JSON.parse(event.body || "{}");
    const { job_id } = body;
    if (!job_id) {
      return {
        statusCode: 400,
        headers,
        body: JSON.stringify({ error: "job_id is required" })
      };
    }
    // Retrieve the job record and verify ownership
    const { data: job, error: jobError } = await supabaseAdmin
      .from("ai_jobs")
      .select("*")
      .eq("id", job_id)
      .eq("owner_user_id", user.id)
      .single();
    if (jobError || !job) {
      return {
        statusCode: 404,
        headers,
        body: JSON.stringify({ error: "Job not found" })
      };
    }
    if (job.status === "running") {
      return {
        statusCode: 409,
        headers,
        body: JSON.stringify({ error: "Job already running" })
      };
    }
    // Update job status to running and queue asynchronous processing
    await supabaseAdmin
      .from("ai_jobs")
      .update({ status: "running", updated_at: new Date().toISOString() })
      .eq("id", job_id);
    processJobAsync(job, user.id).catch((err) => {
      console.error(`[AIJobRunner] Async processing error for ${job_id}:`, err);
    });
    return {
      statusCode: 202,
      headers,
      body: JSON.stringify({
        success: true,
        message: "Job queued for processing",
        job_id
      })
    };
  } catch (err) {
    console.error("Handler Critical Error:", err);
    return {
      statusCode: 500,
      headers,
      body: JSON.stringify({ error: err.message || "Internal server error" })
    };
  }
};

/**
 * Dispatches the job to the appropriate processor based on its type.
 * Catches errors and records them on the job record. On success,
 * updates the job record with the result. This function is run
 * asynchronously and does not block the HTTP response.
 *
 * @param {object} job - The job record from the database
 * @param {string} userId - ID of the job owner
 */
async function processJobAsync(job, userId) {
  const job_id = job.id;
  
  // Create performance tracker at the start of the request
  const perfTracker = createPerformanceTracker();
  console.log(`[AIJobRunner] Created performance tracker: ${perfTracker.requestId} for job ${job_id} (${job.job_type})`);
  
  // Create timing tracker for detailed step-by-step timing
  const timingTracker = createTimingTracker();
  timingTracker.startJob();
  console.log(`[AIJobRunner] Starting job ${job_id} (${job.job_type})`);
  
  let result;
  let error = null;
  try {
    const input = job.input;
    switch (job.job_type) {
      case "batch":
        result = await processBatchJob(input, supabaseAdmin, userId, perfTracker, timingTracker);
        break;
      case "auto_tag":
        result = await processAutoTag(input, supabaseAdmin, perfTracker, timingTracker);
        break;
      case "product_shot":
        result = await processProductShot(input, supabaseAdmin, userId, perfTracker, timingTracker);
        break;
      case "headshot_generate":
        result = await processHeadshotGenerate(input, supabaseAdmin, userId, perfTracker, timingTracker);
        break;
      case "body_shot_generate":
        result = await processBodyShotGenerate(input, supabaseAdmin, userId, perfTracker, timingTracker);
        break;
      case "outfit_suggest":
        result = await processOutfitSuggest(input, supabaseAdmin, userId, perfTracker, timingTracker);
        break;
      case "reference_match":
        result = await processReferenceMatch(input, supabaseAdmin, userId, perfTracker, timingTracker);
        break;
      case "outfit_mannequin":
        result = await processOutfitMannequin(input, supabaseAdmin, userId, perfTracker, timingTracker);
        break;
      case "outfit_render":
        result = await processOutfitRender(input, supabaseAdmin, userId, perfTracker, timingTracker);
        break;
      default:
        throw new Error(`Unknown job type: ${job.job_type}`);
    }
  } catch (err) {
    error = err.message || "Unknown error";
    console.error(`[AIJobRunner] Error processing ${job.job_type} job ${job_id}:`, err);
  }
  
  // Log performance comparison at the end of the request
  perfTracker.logComparison();
  
  // Log detailed timing breakdown
  timingTracker.logBreakdown(job.job_type);
  
  // Build the update payload based on success or failure
  const updateData = {
    updated_at: new Date().toISOString(),
    status: error ? "failed" : "succeeded"
  };
  if (error) {
    updateData.error = error;
  } else {
    updateData.result = result;
  }
  await supabaseAdmin.from("ai_jobs").update(updateData).eq("id", job_id);
}

/**
 * Processes a batch job that runs multiple tasks on the same image in parallel.
 * Downloads the image once and passes it to all tasks to avoid redundant downloads.
 * 
 * Expected input format:
 * {
 *   imageId: string,
 *   tasks: ['product_shot', 'auto_tag'],
 *   wardrobe_item_id: string,
 *   image_ids: string[] (for auto_tag)
 * }
 * 
 * @param {object} input - Batch job input
 * @param {import('@supabase/supabase-js').SupabaseClient} supabase - Supabase client
 * @param {string} userId - ID of the job owner
 * @param {object} perfTracker - Performance tracker
 * @param {object} timingTracker - Timing tracker
 * @returns {Promise<object>} Combined results from all tasks
 */
async function processBatchJob(input, supabase, userId, perfTracker, timingTracker) {
  const { imageId, tasks, wardrobe_item_id, image_ids } = input;
  
  if (!imageId || !tasks || !Array.isArray(tasks) || tasks.length === 0) {
    throw new Error("Batch job requires imageId and tasks array");
  }
  
  if (!wardrobe_item_id) {
    throw new Error("Batch job requires wardrobe_item_id");
  }
  
  console.log(`[BatchJob] Starting batch processing for imageId: ${imageId}, tasks: ${tasks.join(', ')}`);
  
  // Download the image once to a buffer (returns { base64, mimeType } object)
  console.log(`[BatchJob] Downloading image once for shared use...`);
  const imageData = await downloadImageFromStorage(supabase, imageId, timingTracker);
  console.log(`[BatchJob] Image downloaded successfully, length: ${imageData.base64.length}, mimeType: ${imageData.mimeType}`);
  
  // Prepare inputs for both tasks
  const productShotInput = tasks.includes('product_shot') ? {
    image_id: imageId,
    wardrobe_item_id
  } : null;
  
  const autoTagInput = tasks.includes('auto_tag') ? {
    wardrobe_item_id,
    image_ids: image_ids || []
  } : null;
  
  // Validate auto_tag input if needed
  if (tasks.includes('auto_tag')) {
    if (!image_ids || !Array.isArray(image_ids) || image_ids.length === 0) {
      throw new Error("auto_tag task requires image_ids array");
    }
  }
  
  // CRITICAL: Create both promises in the SAME synchronous block - no awaits between them
  // This ensures both async functions start executing immediately and run in parallel
  const batchStart = performance.now();
  console.log(`[BatchJob] Creating both promises simultaneously (parallel execution starts now)...`);
  
  // Create promise array and start BOTH tasks immediately without any await
  const taskPromises = [];
  
  // Create product_shot promise - function starts executing immediately (no await)
  if (productShotInput) {
    console.log(`[BatchJob] Creating product_shot promise (execution starts immediately)...`);
    const productShotPromise = processProductShot(
      productShotInput, 
      supabase, 
      userId, 
      perfTracker, 
      timingTracker, 
      imageData
    ).then(result => {
      const elapsed = ((performance.now() - batchStart) / 1000).toFixed(2);
      console.log(`[BatchJob] product_shot completed in ${elapsed}s`);
      return { task: 'product_shot', result, error: null };
    }).catch(err => {
      const elapsed = ((performance.now() - batchStart) / 1000).toFixed(2);
      console.error(`[BatchJob] product_shot failed after ${elapsed}s:`, err.message);
      return { task: 'product_shot', result: null, error: err.message };
    });
    taskPromises.push(productShotPromise);
  }
  
  // Create auto_tag promise IMMEDIATELY after (no await) - both run in parallel
  if (autoTagInput) {
    console.log(`[BatchJob] Creating auto_tag promise (execution starts immediately, parallel with product_shot)...`);
    const autoTagPromise = processAutoTag(
      autoTagInput, 
      supabase, 
      perfTracker, 
      timingTracker, 
      imageData
    ).then(result => {
      const elapsed = ((performance.now() - batchStart) / 1000).toFixed(2);
      console.log(`[BatchJob] auto_tag completed in ${elapsed}s`);
      return { task: 'auto_tag', result, error: null };
    }).catch(err => {
      const elapsed = ((performance.now() - batchStart) / 1000).toFixed(2);
      console.error(`[BatchJob] auto_tag failed after ${elapsed}s:`, err.message);
      return { task: 'auto_tag', result: null, error: err.message };
    });
    taskPromises.push(autoTagPromise);
  }
  
  // Both promises are now created and executing in parallel
  // Only NOW do we await - Promise.all waits for BOTH to complete
  console.log(`[BatchJob] Both promises created. Awaiting ${taskPromises.length} promises with Promise.all() (both running in parallel)...`);
  const results = await Promise.all(taskPromises);
  
  // Capture end time after Promise.all() completes
  const batchEnd = performance.now();
  const batchDuration = ((batchEnd - batchStart) / 1000).toFixed(2);
  
  // Track the parallel execution time
  if (timingTracker && typeof timingTracker.setBatchAIGenerationTime === 'function') {
    timingTracker.setBatchAIGenerationTime(parseFloat(batchDuration) * 1000); // Convert to ms
  }
  
  console.log(`[BatchJob] All tasks completed in ${batchDuration}s (parallel execution - should be ~max of individual task times, not sum)`);
  
  // Organize results by task name
  const taskResults = {};
  for (const { task, result, error } of results) {
    if (error) {
      taskResults[task] = { error };
      console.error(`[BatchJob] Task ${task} failed:`, error);
    } else {
      taskResults[task] = result;
      console.log(`[BatchJob] Task ${task} completed successfully`);
    }
  }
  
  console.log(`[BatchJob] All tasks completed`);
  return taskResults;
}


===== src/lib/ai-jobs/index.ts =====
/**
 * AI Jobs module - exports all AI job-related functions
 * 
 * Usage:
 * import { createAIJob, pollAIJob, triggerAutoTag } from '@/lib/ai-jobs';
 */

// Re-export from core
export {
  type AIJob,
  createAIJob,
  getAIJob,
  isGeminiPolicyBlockError,
  getActiveJob,
  getRecentJob,
  getOutfitRenderItemLimit,
} from './core';

// Re-export from polling
export {
  pollAIJob,
  pollAIJobWithFinalCheck,
  waitForAIJobCompletion,
  resetCircuitBreaker,
  isCircuitBreakerOpen,
} from './polling';

// Re-export from execution
export {
  triggerAIJobExecution,
  createAndTriggerJob,
} from './execution';

// Re-export from types
export {
  triggerAutoTag,
  applyAutoTagResults,
  triggerProductShot,
  getActiveProductShotJob,
  getRecentProductShotJob,
  triggerHeadshotGenerate,
  triggerBodyShotGenerate,
  getActiveOutfitRenderJob,
  getRecentOutfitRenderJob,
  triggerBatchJob,
  getActiveBatchJob,
  getRecentBatchJob,
} from './types';

