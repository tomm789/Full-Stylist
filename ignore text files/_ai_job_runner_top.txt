"use strict";

// Main Netlify function for processing AI jobs. This handler
// authenticates the request using the Supabase JWT, retrieves the
// referenced job from the database and dispatches processing to the
// appropriate module based on the job type. Each job type is defined
// in a separate module under ./processes to improve modularity.

const { supabaseAdmin } = require("./supabaseClient");
const { createPerformanceTracker, createTimingTracker, downloadImageFromStorage } = require("./utils");
const { processAutoTag } = require("./processes/auto_tag");
const { processProductShot } = require("./processes/product_shot");
const { processHeadshotGenerate } = require("./processes/headshot_generate");
const { processBodyShotGenerate } = require("./processes/body_shot_generate");
const { processOutfitRender } = require("./processes/outfit_render");
const { processOutfitMannequin } = require("./processes/outfit_mannequin");
const { processOutfitSuggest } = require("./processes/outfit_suggest");
const { processReferenceMatch } = require("./processes/reference_match");

/**
 * Netlify function handler. Validates the HTTP method and the
 * Authorization header, retrieves the job from Supabase, and runs it
 * synchronously. Returns 200 on success or 500 on failure after processing completes.
 *
 * @param {object} event - The incoming request context
 * @param {object} context - The Lambda context provided by Netlify
 * @returns {Promise<object>} HTTP response object
 */
exports.handler = async (event, context) => {
  const handlerStartTime = Date.now();
  const headers = {
    "Access-Control-Allow-Origin": "*",
    "Access-Control-Allow-Headers": "Content-Type, Authorization",
    "Access-Control-Allow-Methods": "POST, OPTIONS",
    "Content-Type": "application/json"
  };
  // Handle CORS preflight
  if (event.httpMethod === "OPTIONS") {
    return { statusCode: 200, headers, body: "" };
  }
  if (event.httpMethod !== "POST") {
    return {
      statusCode: 405,
      headers,
      body: JSON.stringify({ error: "Method not allowed" })
    };
  }
  try {
    // Extract and verify JWT from Authorization header
    const authHeader = event.headers.authorization || event.headers.Authorization;
    if (!authHeader || !authHeader.startsWith("Bearer ")) {
      return {
        statusCode: 401,
        headers,
        body: JSON.stringify({ error: "Missing authorization header" })
      };
    }
    const token = authHeader.replace("Bearer ", "");
    const {
      data: { user },
      error: authError
    } = await supabaseAdmin.auth.getUser(token);
    if (authError || !user) {
      return {
        statusCode: 401,
        headers,
        body: JSON.stringify({ error: "Invalid token" })
      };
    }
    // Parse the job ID from the request body
    const body = JSON.parse(event.body || "{}");
    const { job_id } = body;
    if (!job_id) {
      return {
        statusCode: 400,
        headers,
        body: JSON.stringify({ error: "job_id is required" })
      };
    }
    // Retrieve the job record and verify ownership
    const { data: job, error: jobError } = await supabaseAdmin
      .from("ai_jobs")
      .select("*")
      .eq("id", job_id)
      .eq("owner_user_id", user.id)
      .single();
    if (jobError || !job) {
      return {
        statusCode: 404,
        headers,
        body: JSON.stringify({ error: "Job not found" })
      };
    }
    if (job.status === "running") {
      return {
        statusCode: 409,
        headers,
        body: JSON.stringify({ error: "Job already running" })
      };
    }

    console.log("[AIJobRunner] HANDLER START", { job_id, job_type: job.job_type });

    // Update job status to running
    await supabaseAdmin
      .from("ai_jobs")
      .update({ status: "running", updated_at: new Date().toISOString() })
      .eq("id", job_id);

    console.log("[AIJobRunner] BEFORE await processJob", { job_id, job_type: job.job_type });

    let outcome;
    try {
      outcome = await processJobAsync(job, user.id);
    } catch (err) {
      console.error("[AIJobRunner] processJobAsync threw:", err);
      outcome = { result: null, error: err.message || "Unknown error" };
    }

    console.log("[AIJobRunner] AFTER await processJob", { job_id, job_type: job.job_type });

    const duration_ms = Date.now() - handlerStartTime;
    const status = outcome.error ? "failed" : "succeeded";
    console.log("[AIJobRunner] RETURNING", { job_id, status, duration_ms });

    if (outcome.error) {
      return {
        statusCode: 500,
        headers,
        body: JSON.stringify({ success: false, job_id, error: outcome.error })
      };
    }
    return {
      statusCode: 200,
      headers,
      body: JSON.stringify({ success: true, job_id, status: "succeeded" })
    };
  } catch (err) {
    console.error("Handler Critical Error:", err);
    return {
      statusCode: 500,
      headers,
      body: JSON.stringify({ error: err.message || "Internal server error" })
    };
  }
};

/**
 * Dispatches the job to the appropriate processor based on its type.
 * Fully awaits the selected processor. Updates the job record exactly once
 * after processing completes. Returns { result, error } for handler response.
 *
 * @param {object} job - The job record from the database
 * @param {string} userId - ID of the job owner
 * @returns {Promise<{result: object|null, error: string|null}>}
 */
async function processJobAsync(job, userId) {
  const job_id = job.id;

  // Create performance tracker at the start of the request
  const perfTracker = createPerformanceTracker();
  console.log(`[AIJobRunner] Created performance tracker: ${perfTracker.requestId} for job ${job_id} (${job.job_type})`);

  // Create timing tracker for detailed step-by-step timing
  const timingTracker = createTimingTracker();
  timingTracker.startJob();
  console.log(`[AIJobRunner] Starting job ${job_id} (${job.job_type})`);

  let result;
  let error = null;
  try {
    const input = job.input;
    switch (job.job_type) {
      case "batch":
        result = await processBatchJob(input, supabaseAdmin, userId, perfTracker, timingTracker, job_id);
        break;
      case "auto_tag":
        result = await processAutoTag(input, supabaseAdmin, perfTracker, timingTracker, null, job_id);
        break;
      case "product_shot":
        result = await processProductShot(input, supabaseAdmin, userId, perfTracker, timingTracker, null, job_id);
        break;
      case "headshot_generate":
        result = await processHeadshotGenerate(input, supabaseAdmin, userId, perfTracker, timingTracker, job_id);
        break;
      case "body_shot_generate":
        result = await processBodyShotGenerate(input, supabaseAdmin, userId, perfTracker, timingTracker, job_id);
        break;
      case "outfit_suggest":
        result = await processOutfitSuggest(input, supabaseAdmin, userId, perfTracker, timingTracker, job_id);
        break;
      case "reference_match":
        result = await processReferenceMatch(input, supabaseAdmin, userId, perfTracker, timingTracker, job_id);
        break;
      case "outfit_mannequin":
        result = await processOutfitMannequin(input, supabaseAdmin, userId, perfTracker, timingTracker, job_id);
        break;
      case "outfit_render":
        result = await processOutfitRender(input, supabaseAdmin, userId, perfTracker, timingTracker, job_id);
        break;
      default:
        throw new Error(`Unknown job type: ${job.job_type}`);
    }
  } catch (err) {
    error = err.message || "Unknown error";
    console.error(`[AIJobRunner] Error processing ${job.job_type} job ${job_id}:`, err);
  }

  // Log performance comparison at the end of the request
  perfTracker.logComparison();

  // Log detailed timing breakdown
  timingTracker.logBreakdown(job.job_type);

  // Build the update payload based on success or failure (exactly once after processing)
  const updateData = {
    updated_at: new Date().toISOString(),
    status: error ? "failed" : "succeeded"
  };
  if (error) {
    updateData.error = error;
  } else {
    updateData.result = result;
  }
  await supabaseAdmin.from("ai_jobs").update(updateData).eq("id", job_id);

  return { result: error ? null : result, error };
}

/**
 * Processes a batch job that runs multiple tasks on the same image in parallel.
 * Downloads the image once and passes it to all tasks to avoid redundant downloads.
 * 
 * Expected input format:
 * {
 *   imageId: string,
 *   tasks: ['product_shot', 'auto_tag'],
 *   wardrobe_item_id: string,
 *   image_ids: string[] (for auto_tag)
 * }
 * 
 * @param {object} input - Batch job input
 * @param {import('@supabase/supabase-js').SupabaseClient} supabase - Supabase client
 * @param {string} userId - ID of the job owner
 * @param {object} perfTracker - Performance tracker
 * @param {object} timingTracker - Timing tracker
 * @returns {Promise<object>} Combined results from all tasks
 */
async function processBatchJob(input, supabase, userId, perfTracker, timingTracker, jobId = null) {
  const { imageId, tasks, wardrobe_item_id, image_ids } = input;
  
  if (!imageId || !tasks || !Array.isArray(tasks) || tasks.length === 0) {
    throw new Error("Batch job requires imageId and tasks array");
  }
  
  if (!wardrobe_item_id) {
    throw new Error("Batch job requires wardrobe_item_id");
  }
  
  console.log(`[BatchJob] Starting batch processing for imageId: ${imageId}, tasks: ${tasks.join(', ')}`);
